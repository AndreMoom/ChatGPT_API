{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conectarse a la API de CHATGPT\n",
    "\n",
    "**Tutorial basado en openai-cookbook de OpenAI.**\n",
    "\n",
    "ChatGPT funciona con gpt-3.5-turbo, el modelo más avanzado de OpenAI.\n",
    "\n",
    "Puede crear sus propias aplicaciones con gpt-3.5-turbo utilizando la API de OpenAI.\n",
    "\n",
    "Los modelos de chat toman una serie de mensajes como entrada y devuelven un mensaje escrito por IA como salida.\n",
    "\n",
    "Esta guía ilustra el formato de chat con algunos ejemplos de llamadas API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar la librería de openai\n",
    "\n",
    "En caso no lo tengas instalada, la puedes instalar con el siguiente código para ejecutar en la terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería\n",
    "import openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ejemplo de de llamada de CHATGPT\n",
    "\n",
    "La API de ChatGPT, requiere 2 entradas:\n",
    "\n",
    "* model (modelo): Es el nombre del modelo que utilizaremos (ejemplo: gpt-3.5-turbo).\n",
    "\n",
    "* messages (mensaje): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente eficiente.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"¿Quién esta ahí?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Naranja.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrae solo la respuesta:\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluso las tareas que no se basan en conversaciones pueden encajar en el formato de chat, colocando la instrucción en el primer mensaje del usuario.\n",
    "\n",
    "Por ejemplo, para pedirle al modelo que explique la programación asíncrona al estilo del pirata Barbanegra, podemos estructurar la conversación de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with a system message\n",
    "# ejemplo con un mensaje con rol de sistema.\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente eficiente.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explique la programación asíncrona al estilo del pirata Barbanegra.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example without a system message\n",
    "# ejemplo sin un mensaje con rol de sistema\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explique la programación asíncrona al estilo del pirata Barbanegra.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consejos para instruir gpt-3.5-turbo-0301\n",
    "\n",
    "Las mejores prácticas para instruir modelos pueden cambiar de una versión de modelo a otra. El consejo que sigue se aplica a gpt-3.5-turbo-0301 y es posible que no se aplique a modelos futuros.\n",
    "\n",
    "Mensajes del sistema\n",
    "El mensaje del sistema se puede utilizar para preparar al asistente con diferentes personalidades o comportamientos.\n",
    "\n",
    "Sin embargo, el modelo generalmente no presta tanta atención al mensaje del sistema y, por lo tanto, recomendamos colocar instrucciones importantes en el mensaje del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo de un mensaje del sistema que prepara al asistente para explicar conceptos en gran profundidad\n",
    "respuesta = openai.ChatCompletion.create(\n",
    "     model=MODEL,\n",
    "     menssages=[\n",
    "         {\"role\": \"system\", \"content\": \"Eres un asistente de enseñanza amable y servicial. Explicas conceptos en gran profundidad usando términos simples y das ejemplos para ayudar a las personas a aprender. Al final de cada explicación, hacer una pregunta para verificar la comprensión\"},\n",
    "         {\"role\": \"user\", \"content\": \"¿Puedes explicar cómo funcionan las fracciones?\"},\n",
    "     ],\n",
    "     temperature=0,\n",
    ")\n",
    "\n",
    "print(respuesta[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo de un mensaje del sistema que prepara al asistente para dar respuestas breves y directas\n",
    "respuesta = openai.ChatCompletion.create(\n",
    "     model=MODEL,\n",
    "     mensages=[\n",
    "         {\"role\": \"system\", \"content\": \"Eres un asistente lacónico. Respondes con respuestas breves y directas sin elaboración.\"},\n",
    "         {\"role\": \"user\", \"content\": \"¿Puedes explicar cómo funcionan las fracciones?\"},\n",
    "     ],\n",
    "     temperature=0,\n",
    ")\n",
    "\n",
    "print(respuesta[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicación de pocos disparos\n",
    "En algunos casos, es más fácil mostrarle al modelo lo que desea en lugar de decirle al modelo lo que desea.\n",
    "\n",
    "Una forma de mostrarle al modelo lo que quiere es con mensajes de ejemplo falsos.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo de una conversación falsa de pocas tomas para preparar al modelo para que traduzca la jerga comercial a un discurso más simple\n",
    "respuesta = openai.ChatCompletion.create(\n",
    "     model=MODEL,\n",
    "     menssages=[\n",
    "         {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue patrones.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Ayúdame a traducir la siguiente jerga corporativa al inglés sencillo.\"},\n",
    "         {\"role\": \"asistent\", \"content\": \"¡Claro, me encantaría!\"},\n",
    "         {\"role\": \"user\", \"content\": \"Nuevas sinergias ayudarán a impulsar el crecimiento de primera línea.\"},\n",
    "         {\"role\": \"asistent\", \"content\": \"Las cosas que funcionan bien juntas aumentarán los ingresos.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Regresemos cuando tengamos más ancho de banda para tocar la base de oportunidades para aumentar el apalancamiento.\"},\n",
    "         {\"role\": \"assistant\", \"content\": \"Hablemos más tarde, cuando estemos menos ocupados, sobre cómo hacerlo mejor.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Este pivote tardío significa que no tenemos tiempo para hervir el océano para la entrega del cliente.\"},\n",
    "     ],\n",
    "     temperature=0,\n",
    ")\n",
    "\n",
    "print(respuesta[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ayudar a aclarar que los mensajes de ejemplo no son parte de una conversación real, y el modelo no debe volver a consultarlos, puede establecer el campo de nombre de los mensajes del sistema en ejemplo_usuario y ejemplo_asistente.\n",
    "\n",
    "Transformando el ejemplo anterior de pocos disparos, podríamos escribir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El ejemplo de traducción de la jerga comercial, pero con nombres de ejemplo para los mensajes de ejemplo\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "         {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue patrones y traduce la jerga corporativa al inglés sencillo.\"},\n",
    "         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Nuevas sinergias ayudarán a impulsar el crecimiento de primera línea.\"},\n",
    "         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Las cosas que funcionan bien juntas aumentarán los ingresos.\"},\n",
    "         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Regresemos cuando tengamos más ancho de banda para tocar la base de las oportunidades para aumentar el apalancamiento.\"},\n",
    "         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Hablemos más tarde, cuando estemos menos ocupados, sobre cómo hacerlo mejor.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Este pivote tardío significa que no tenemos tiempo para hervir el océano para la entrega del cliente.\"},\n",
    "     ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo de un mensaje del sistema que prepara al asistente para dar respuestas breves y directas\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente lacónico. Respondes con respuestas breves y directas sin elaboración.\"},\n",
    "        {\"role\": \"user\", \"content\": \"¿Puedes explicar cómo funcionan las fracciones?\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicación de pocos disparos\n",
    "En algunos casos, es más fácil mostrarle al modelo lo que desea en lugar de decirle al modelo lo que desea.\n",
    "\n",
    "Una forma de mostrarle al modelo lo que quiere es con mensajes de ejemplo falsos.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ejemplo de una conversación falsa de pocas tomas para preparar al modelo para que traduzca la jerga comercial a un discurso más simple\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue patrones.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Ayúdame a traducir la siguiente jerga corporativa al inglés sencillo.\"},\n",
    "         {\"role\": \"asistente\", \"content\": \"¡Claro, me encantaría!\"},\n",
    "         {\"role\": \"user\", \"content\": \"Nuevas sinergias ayudarán a impulsar el crecimiento de primera línea.\"},\n",
    "         {\"role\": \"asistente\", \"content\": \"Las cosas que funcionan bien juntas aumentarán los ingresos.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Regresemos cuando tengamos más ancho de banda para tocar la base de oportunidades para aumentar el apalancamiento.\"},\n",
    "         {\"role\": \"assistant\", \"content\": \"Hablemos más tarde, cuando estemos menos ocupados, sobre cómo hacerlo mejor.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Este pivote tardío significa que no tenemos tiempo para hervir el océano para la entrega del cliente.\"},],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ayudar a aclarar que los mensajes de ejemplo no son parte de una conversación real, y el modelo no debe volver a consultarlos, puede establecer el campo de nombre de los mensajes del sistema en ejemplo_usuario y ejemplo_asistente.\n",
    "\n",
    "Transformando el ejemplo anterior de pocos disparos, podríamos escribir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El ejemplo de traducción de la jerga comercial, pero con nombres de ejemplo para los mensajes de ejemplo\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "         {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue patrones y traduce la jerga corporativa al inglés sencillo.\"},\n",
    "         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Nuevas sinergias ayudarán a impulsar el crecimiento de primera línea.\"},\n",
    "         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Las cosas que funcionan bien juntas aumentarán los ingresos.\"},\n",
    "         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Regresemos cuando tengamos más ancho de banda para tocar la base de las oportunidades para aumentar el apalancamiento.\"},\n",
    "         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Hablemos más tarde, cuando estemos menos ocupados, sobre cómo hacerlo mejor.\"},\n",
    "         {\"role\": \"user\", \"content\": \"Este pivote tardío significa que no tenemos tiempo para hervir el océano para la entrega del cliente.\"},\n",
    "     ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No todos los intentos de conversaciones de ingeniería tendrán éxito al principio.\n",
    "\n",
    "Si sus primeros intentos fallan, no tenga miedo de experimentar con diferentes formas de preparar o acondicionar el modelo.\n",
    "\n",
    "Como ejemplo, un desarrollador descubrió un aumento en la precisión cuando insertó un mensaje de usuario que decía \"Buen trabajo hasta ahora, estos han sido perfectos\" para ayudar a condicionar el modelo para que brinde respuestas de mayor calidad.\n",
    "\n",
    "Para obtener más ideas sobre cómo aumentar la confiabilidad de los modelos, considere leer nuestra guía sobre técnicas para aumentar la confiabilidad (https://github.com/openai/openai-cookbook/blob/347e54c76ce969eac3495427fe003fc6626765d0/techniques_to_improve_reliability.md). Fue escrito para modelos que no son de chat, pero muchos de sus principios aún se aplican."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contar fichas\n",
    "Cuando envía su solicitud, la API transforma los mensajes en una secuencia de tokens (fichas).\n",
    "\n",
    "El número de fichas utilizadas afecta:\n",
    "\n",
    "* el costo de la solicitud\n",
    "* el tiempo que tarda en generar la respuesta\n",
    "* cuando la respuesta deja de alcanzar el límite máximo de tokens (4096 para gpt-3.5-turbo)\n",
    "\n",
    "A partir del 1 de marzo de 2023, puede usar la siguiente función para contar la cantidad de tokens que usará una lista de mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "     {\"role\": \"system\", \"content\": \"Eres un asistente útil que sigue patrones y traduce la jerga corporativa al inglés sencillo.\"},\n",
    "     {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Nuevas sinergias ayudarán a impulsar el crecimiento de primera línea.\"},\n",
    "     {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Las cosas que funcionan bien juntas aumentarán los ingresos.\"},\n",
    "     {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Regresemos cuando tengamos más ancho de banda para tocar la base de las oportunidades para aumentar el apalancamiento.\"},\n",
    "     {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Hablemos más tarde, cuando estemos menos ocupados, sobre cómo hacerlo mejor.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Este pivote tardío significa que no tenemos tiempo para hervir el océano para la entrega del cliente.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example token count from the function defined above\n",
    "print(f\"{num_tokens_from_messages(messages)} prompt tokens counted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example token count from the OpenAI API\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35aabcdafb5198b47c73ee53df070bb7b723e0e5f28a523cf444b3656a21e3ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
